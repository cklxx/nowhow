import asyncio
import aiohttp
from typing import Dict, List, Any, Optional
from bs4 import BeautifulSoup, Comment
import feedparser
from langchain_core.messages import HumanMessage
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
import random
from urllib.parse import urljoin, urlparse
from datetime import datetime

from .base_agent import BaseAgent, AgentState
from utils.file_storage import FileStorage
from utils.source_manager import SourceManager
from services.ai_analyzer import AIWebAnalyzer
from services.mock_auth_finder import MockAuthFinder
from models.source_config import (
    SourceConfig, SourceType, ContentType, CrawlResult, 
    AIAnalysisRequest, MockAuthRequest, AuthType
)

class EnhancedCrawlerAgent(BaseAgent):
    """å¢å¼ºç‰ˆçˆ¬è™«ä»£ç†ï¼Œæ”¯æŒç”¨æˆ·è‡ªå®šä¹‰ä¿¡æºå’ŒAIæ™ºèƒ½é…ç½®"""
    
    def __init__(self, api_key: str = None):
        super().__init__("EnhancedCrawlerAgent")
        self.file_storage = FileStorage()
        self.source_manager = SourceManager()
        self.ai_analyzer = AIWebAnalyzer(api_key)
        self.mock_auth_finder = MockAuthFinder()
        
        # ç”¨æˆ·ä»£ç†æ± 
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:89.0) Gecko/20100101 Firefox/89.0"
        ]
    
    async def execute(self, state: AgentState) -> AgentState:
        """æ‰§è¡Œå¢å¼ºç‰ˆçˆ¬è™«ä»»åŠ¡"""
        try:
            state.current_step = "enhanced_crawling"
            
            # è·å–æ‰€æœ‰æ¿€æ´»çš„ä¿¡æº
            sources = self.source_manager.get_active_sources()
            
            # åˆå§‹åŒ–è¿›åº¦è·Ÿè¸ª
            state.progress = {
                "current_step": "enhanced_crawling",
                "total_sources": len(sources),
                "completed_sources": 0,
                "current_source": "",
                "status": "starting",
                "sources_detail": [],
                "rss_articles": 0,
                "web_pages": 0,
                "total_items": 0
            }
            
            print(f"ğŸ¯ å¼€å§‹å¢å¼ºç‰ˆå†…å®¹æŠ“å–...")
            print(f"ğŸ“Š å‘ç° {len(sources)} ä¸ªé…ç½®çš„ä¿¡æº")
            print("=" * 60)
            
            all_content = []
            
            for i, source in enumerate(sources, 1):
                try:
                    state.progress["current_source"] = str(source.url)
                    state.progress["completed_sources"] = i - 1
                    
                    print(f"  [{i}/{len(sources)}] å¤„ç†ä¿¡æº: {source.name}")
                    print(f"    URL: {source.url}")
                    print(f"    ç±»å‹: {source.type.value} | å†…å®¹ç±»å‹: {source.content_type.value}")
                    
                    # è®°å½•å¼€å§‹æ—¶é—´
                    start_time = time.time()
                    
                    # æ ¹æ®ä¿¡æºç±»å‹è¿›è¡ŒæŠ“å–
                    if source.type == SourceType.RSS:
                        content = await self._crawl_rss_source(source, state)
                    else:
                        content = await self._crawl_web_source(source, state)
                    
                    # è®¡ç®—å“åº”æ—¶é—´
                    response_time = time.time() - start_time
                    
                    if content:
                        all_content.extend(content)
                        
                        # ä¿å­˜æŠ“å–ç»“æœ
                        for item in content:
                            result = CrawlResult(
                                source_id=source.id,
                                url=item.get("url", str(source.url)),
                                success=True,
                                title=item.get("title"),
                                content=item.get("content"),
                                summary=item.get("summary"),
                                author=item.get("author"),
                                publish_date=item.get("published"),
                                tags=item.get("tags", []),
                                response_time=response_time,
                                content_length=len(item.get("content", "")),
                                content_quality=item.get("quality_score", 0.8)
                            )
                            self.source_manager.save_crawl_result(result)
                        
                        print(f"    âœ… æˆåŠŸè·å– {len(content)} é¡¹å†…å®¹")
                        
                        # æ›´æ–°è¿›åº¦
                        if source.type == SourceType.RSS:
                            state.progress["rss_articles"] += len(content)
                        else:
                            state.progress["web_pages"] += len(content)
                        
                        state.progress["total_items"] = len(all_content)
                    
                    else:
                        print(f"    âš ï¸ æœªè·å–åˆ°å†…å®¹")
                        
                        # è®°å½•å¤±è´¥ç»“æœ
                        result = CrawlResult(
                            source_id=source.id,
                            url=str(source.url),
                            success=False,
                            error_type="no_content",
                            error_message="æœªèƒ½æå–åˆ°æœ‰æ•ˆå†…å®¹",
                            response_time=response_time
                        )
                        self.source_manager.save_crawl_result(result)
                    
                    # æ›´æ–°æºè¯¦æƒ…
                    source_detail = {
                        "id": source.id,
                        "name": source.name,
                        "url": str(source.url),
                        "type": source.type.value,
                        "content_type": source.content_type.value,
                        "status": "completed" if content else "no_content",
                        "items_count": len(content) if content else 0,
                        "response_time": response_time,
                        "ai_analyzed": source.ai_analyzed
                    }
                    
                    if content:
                        source_detail["sample_titles"] = [
                            item.get("title", "æ— æ ‡é¢˜")[:60] + ("..." if len(item.get("title", "")) > 60 else "")
                            for item in content[:3]
                        ]
                    
                    state.progress["sources_detail"].append(source_detail)
                    
                    # æ·»åŠ è¯·æ±‚é—´éš”
                    if i < len(sources):
                        delay = source.crawl_config.delay
                        if delay > 0:
                            await asyncio.sleep(delay)
                
                except Exception as e:
                    print(f"    âŒ å¤„ç†ä¿¡æºå¤±è´¥: {e}")
                    
                    # è®°å½•é”™è¯¯ç»“æœ
                    result = CrawlResult(
                        source_id=source.id,
                        url=str(source.url),
                        success=False,
                        error_type="crawl_error",
                        error_message=str(e)
                    )
                    self.source_manager.save_crawl_result(result)
                    
                    # æ·»åŠ é”™è¯¯è¯¦æƒ…åˆ°è¿›åº¦
                    state.progress["sources_detail"].append({
                        "id": source.id,
                        "name": source.name,
                        "url": str(source.url),
                        "type": source.type.value,
                        "status": "error",
                        "error": str(e)
                    })
                    continue
            
            # å®ŒæˆæŠ“å–
            state.progress["completed_sources"] = len(sources)
            state.progress["status"] = "completed"
            
            print("=" * 60)
            print(f"âœ… å¢å¼ºç‰ˆæŠ“å–å®Œæˆ!")
            print(f"ğŸ“Š æ€»è®¡è·å–: {len(all_content)} é¡¹å†…å®¹")
            print(f"ğŸ“¡ RSSæ–‡ç« : {state.progress['rss_articles']} ç¯‡")
            print(f"ğŸŒ ç½‘é¡µå†…å®¹: {state.progress['web_pages']} ä¸ª")
            print("=" * 60)
            
            # ç»„åˆæ‰€æœ‰å†…å®¹
            enhanced_content = {
                "enhanced_content": all_content,
                "crawl_timestamp": datetime.now().timestamp(),
                "progress": state.progress,
                "sources_used": len(sources),
                "total_items": len(all_content),
                "stats": {
                    "rss_articles": state.progress["rss_articles"],
                    "web_pages": state.progress["web_pages"],
                    "total_sources": len(sources),
                    "successful_sources": len([s for s in state.progress["sources_detail"] if s.get("status") == "completed"])
                }
            }
            
            state.data["crawled_content"] = enhanced_content
            
            # ä¿å­˜çˆ¬å–å†…å®¹åˆ°æ–‡ä»¶
            workflow_id = state.data.get("workflow_id", "default")
            saved_path = self.file_storage.save_crawled_content(enhanced_content, workflow_id)
            state.data["crawled_content_file"] = saved_path
            
            state.messages.append(
                HumanMessage(content=f"Enhanced crawling completed: {len(all_content)} items from {len(sources)} sources")
            )
            
            return state
            
        except Exception as e:
            state.error = f"Enhanced crawler error: {str(e)}"
            print(f"âŒ å¢å¼ºç‰ˆçˆ¬è™«é”™è¯¯: {e}")
            return state
    
    async def _crawl_rss_source(self, source: SourceConfig, state: AgentState) -> List[Dict[str, Any]]:
        """æŠ“å–RSSä¿¡æº"""
        try:
            # è®¾ç½®è¯·æ±‚å¤´
            headers = dict(source.auth_config.headers) if source.auth_config.headers else {}
            if not headers.get("User-Agent"):
                headers["User-Agent"] = random.choice(self.user_agents)
            
            # è§£æRSS
            feed = feedparser.parse(str(source.url))
            
            articles = []
            max_items = min(10, len(feed.entries))  # æœ€å¤šå–10ç¯‡æ–‡ç« 
            
            for entry in feed.entries[:max_items]:
                article = {
                    "title": entry.get("title", ""),
                    "url": entry.get("link", ""),
                    "summary": entry.get("summary", ""),
                    "content": entry.get("content", [{}])[0].get("value", "") if entry.get("content") else "",
                    "author": entry.get("author", ""),
                    "published": entry.get("published", ""),
                    "tags": [tag.get("term", "") for tag in entry.get("tags", [])],
                    "source": str(source.url),
                    "source_name": source.name,
                    "type": "rss",
                    "content_type": source.content_type.value
                }
                
                # å¦‚æœæ²¡æœ‰å†…å®¹ï¼Œå°è¯•è·å–å…¨æ–‡
                if not article["content"] and article["url"]:
                    try:
                        full_content = await self._fetch_full_article(article["url"], source)
                        if full_content:
                            article["content"] = full_content
                    except Exception as e:
                        print(f"      è·å–å…¨æ–‡å¤±è´¥: {e}")
                
                articles.append(article)
            
            return articles
            
        except Exception as e:
            print(f"    RSSæŠ“å–é”™è¯¯: {e}")
            return []
    
    async def _crawl_web_source(self, source: SourceConfig, state: AgentState) -> List[Dict[str, Any]]:
        """æŠ“å–ç½‘é¡µä¿¡æº"""
        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦AIåˆ†æ
            if not source.ai_analyzed:
                print(f"    ğŸ” æ‰§è¡ŒAIç»“æ„åˆ†æ...")
                await self._analyze_with_ai(source)
            
            # è·å–ç½‘é¡µå†…å®¹
            if source.crawl_config.render_js:
                content = await self._fetch_with_selenium(source)
            else:
                content = await self._fetch_with_aiohttp(source)
            
            if not content:
                return []
            
            # è§£æå†…å®¹
            soup = BeautifulSoup(content, 'html.parser')
            
            # ä½¿ç”¨é…ç½®çš„é€‰æ‹©å™¨æå–å†…å®¹
            extracted_items = self._extract_content_with_selectors(soup, source)
            
            return extracted_items
            
        except Exception as e:
            print(f"    ç½‘é¡µæŠ“å–é”™è¯¯: {e}")
            return []
    
    async def _analyze_with_ai(self, source: SourceConfig):
        """ä½¿ç”¨AIåˆ†æç½‘é¡µç»“æ„"""
        try:
            request = AIAnalysisRequest(
                url=source.url,
                content_type=source.content_type
            )
            
            analysis_result = await self.ai_analyzer.analyze_webpage(request)
            
            # æ›´æ–°æºé…ç½®
            source.content_selectors = analysis_result.suggested_selectors
            source.ai_analyzed = True
            source.ai_analysis_time = datetime.now()
            source.ai_confidence = analysis_result.confidence
            source.ai_suggestions = analysis_result.recommendations
            
            # æ›´æ–°JSæ¸²æŸ“éœ€æ±‚
            if analysis_result.requires_js:
                source.crawl_config.render_js = True
            
            # å¦‚æœéœ€è¦è®¤è¯ï¼Œå°è¯•æŸ¥æ‰¾mocké…ç½®
            if analysis_result.requires_auth:
                print(f"      ğŸ” æ£€æµ‹åˆ°éœ€è¦è®¤è¯ï¼ŒæŸ¥æ‰¾mocké…ç½®...")
                await self._find_mock_auth(source)
            
            # ä¿å­˜æ›´æ–°çš„é…ç½®
            self.source_manager.save_source(source)
            
            print(f"      âœ… AIåˆ†æå®Œæˆï¼Œç½®ä¿¡åº¦: {analysis_result.confidence:.2f}")
            
        except Exception as e:
            print(f"      âš ï¸ AIåˆ†æå¤±è´¥: {e}")
    
    async def _find_mock_auth(self, source: SourceConfig):
        """æŸ¥æ‰¾mockè®¤è¯é…ç½®"""
        try:
            domain = urlparse(str(source.url)).netloc
            
            request = MockAuthRequest(
                url=source.url,
                site_domain=domain,
                auth_type=AuthType.COOKIE
            )
            
            mock_result = await self.mock_auth_finder.find_mock_auth(request)
            
            if mock_result.found and mock_result.auth_config:
                # æ›´æ–°è®¤è¯é…ç½®
                source.auth_config = mock_result.auth_config
                print(f"      ğŸ”‘ æ‰¾åˆ°è®¤è¯é…ç½®: {', '.join(mock_result.sources)}")
                
                # å¦‚æœåŒ…å«placeholderï¼Œç»™å‡ºæç¤º
                if any("placeholder" in str(v) for v in source.auth_config.headers.values()) or \
                   any("placeholder" in str(v) for v in source.auth_config.cookies.values()):
                    print(f"      âš ï¸ è®¤è¯é…ç½®åŒ…å«placeholderï¼Œéœ€è¦æ‰‹åŠ¨æ›¿æ¢çœŸå®å€¼")
            
        except Exception as e:
            print(f"      âš ï¸ æŸ¥æ‰¾è®¤è¯é…ç½®å¤±è´¥: {e}")
    
    async def _fetch_with_aiohttp(self, source: SourceConfig) -> Optional[str]:
        """ä½¿ç”¨aiohttpè·å–ç½‘é¡µå†…å®¹"""
        headers = dict(source.auth_config.headers) if source.auth_config.headers else {}
        cookies = dict(source.auth_config.cookies) if source.auth_config.cookies else {}
        
        if not headers.get("User-Agent"):
            if source.crawl_config.random_user_agent:
                headers["User-Agent"] = random.choice(self.user_agents)
            else:
                headers["User-Agent"] = source.crawl_config.user_agent or self.user_agents[0]
        
        timeout = aiohttp.ClientTimeout(total=source.crawl_config.timeout)
        
        async with aiohttp.ClientSession(timeout=timeout) as session:
            for attempt in range(source.crawl_config.retry_count):
                try:
                    async with session.get(
                        str(source.url),
                        headers=headers,
                        cookies=cookies,
                        proxy=source.crawl_config.proxy
                    ) as response:
                        if response.status == 200:
                            content = await response.text(encoding=source.crawl_config.encoding)
                            return content
                        else:
                            print(f"      HTTP {response.status} (å°è¯• {attempt + 1}/{source.crawl_config.retry_count})")
                
                except Exception as e:
                    print(f"      è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{source.crawl_config.retry_count}): {e}")
                    if attempt < source.crawl_config.retry_count - 1:
                        await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
        
        return None
    
    async def _fetch_with_selenium(self, source: SourceConfig) -> Optional[str]:
        """ä½¿ç”¨Seleniumè·å–éœ€è¦JSæ¸²æŸ“çš„ç½‘é¡µå†…å®¹"""
        options = Options()
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        
        # è®¾ç½®User-Agent
        user_agent = source.auth_config.headers.get("User-Agent")
        if not user_agent:
            user_agent = random.choice(self.user_agents) if source.crawl_config.random_user_agent else self.user_agents[0]
        options.add_argument(f'--user-agent={user_agent}')
        
        driver = None
        try:
            driver = webdriver.Chrome(options=options)
            driver.set_page_load_timeout(source.crawl_config.timeout)
            
            # è®¾ç½®cookies
            if source.auth_config.cookies:
                driver.get(str(source.url))
                for name, value in source.auth_config.cookies.items():
                    if "placeholder" not in value:
                        driver.add_cookie({"name": name, "value": value})
            
            # åŠ è½½é¡µé¢
            driver.get(str(source.url))
            
            # ç­‰å¾…ç‰¹å®šå…ƒç´ åŠ è½½
            if source.crawl_config.wait_for_selector:
                try:
                    WebDriverWait(driver, 10).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, source.crawl_config.wait_for_selector))
                    )
                except Exception:
                    print(f"      ç­‰å¾…å…ƒç´ è¶…æ—¶: {source.crawl_config.wait_for_selector}")
            else:
                # é»˜è®¤ç­‰å¾…é¡µé¢åŠ è½½
                time.sleep(3)
            
            return driver.page_source
            
        except Exception as e:
            print(f"      SeleniumæŠ“å–å¤±è´¥: {e}")
            return None
        
        finally:
            if driver:
                driver.quit()
    
    def _extract_content_with_selectors(self, soup: BeautifulSoup, source: SourceConfig) -> List[Dict[str, Any]]:
        """ä½¿ç”¨é€‰æ‹©å™¨æå–å†…å®¹"""
        items = []
        selectors = source.content_selectors
        
        try:
            # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
            for exclude_selector in selectors.exclude_selectors:
                for element in soup.select(exclude_selector):
                    element.decompose()
            
            # ç§»é™¤æ³¨é‡Š
            for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):
                comment.extract()
            
            # æ ¹æ®é¡µé¢ç±»å‹å†³å®šæå–ç­–ç•¥
            if source.content_type in [ContentType.RESEARCH, ContentType.NEWS]:
                # ç ”ç©¶è®ºæ–‡æˆ–æ–°é—»åˆ—è¡¨é¡µé¢ï¼Œæå–å¤šä¸ªé¡¹ç›®
                items = self._extract_multiple_items(soup, selectors, source)
            else:
                # å•ç¯‡æ–‡ç« æˆ–åšå®¢é¡µé¢
                item = self._extract_single_item(soup, selectors, source)
                if item:
                    items = [item]
            
            # å†…å®¹è´¨é‡è¿‡æ»¤
            filtered_items = []
            for item in items:
                content_length = len(item.get("content", ""))
                if (content_length >= source.crawl_config.min_content_length and 
                    content_length <= source.crawl_config.max_content_length):
                    
                    # è®¡ç®—è´¨é‡åˆ†æ•°
                    item["quality_score"] = self._calculate_quality_score(item)
                    filtered_items.append(item)
            
            return filtered_items
            
        except Exception as e:
            print(f"      å†…å®¹æå–å¤±è´¥: {e}")
            return []
    
    def _extract_multiple_items(self, soup: BeautifulSoup, selectors, source: SourceConfig) -> List[Dict[str, Any]]:
        """æå–å¤šä¸ªå†…å®¹é¡¹ç›®ï¼ˆç”¨äºåˆ—è¡¨é¡µé¢ï¼‰"""
        items = []
        
        # æŸ¥æ‰¾å†…å®¹å®¹å™¨
        content_containers = []
        
        # å°è¯•ä¸åŒçš„å®¹å™¨é€‰æ‹©å™¨
        container_selectors = [
            "article", ".article", ".post", ".paper", ".entry",
            ".list-item", ".content-item", "[itemtype*='Article']"
        ]
        
        for selector in container_selectors:
            containers = soup.select(selector)
            if containers:
                content_containers = containers
                break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å®¹å™¨ï¼Œå°è¯•åŸºäºæ ‡é¢˜æŸ¥æ‰¾
        if not content_containers and selectors.title:
            title_elements = soup.select(selectors.title)
            content_containers = [elem.find_parent() for elem in title_elements if elem.find_parent()]
        
        # æå–æ¯ä¸ªå®¹å™¨çš„å†…å®¹
        for container in content_containers[:10]:  # é™åˆ¶æœ€å¤š10ä¸ªé¡¹ç›®
            try:
                item = self._extract_item_from_container(container, selectors, source)
                if item:
                    items.append(item)
            except Exception as e:
                print(f"        æå–é¡¹ç›®å¤±è´¥: {e}")
                continue
        
        return items
    
    def _extract_single_item(self, soup: BeautifulSoup, selectors, source: SourceConfig) -> Optional[Dict[str, Any]]:
        """æå–å•ä¸ªå†…å®¹é¡¹ç›®"""
        try:
            item = {
                "url": str(source.url),
                "source": str(source.url),
                "source_name": source.name,
                "type": "webpage",
                "content_type": source.content_type.value
            }
            
            # æå–å„ä¸ªå­—æ®µ
            if selectors.title:
                title_elem = soup.select_one(selectors.title)
                if title_elem:
                    item["title"] = title_elem.get_text().strip()
            
            if selectors.content:
                content_elem = soup.select_one(selectors.content)
                if content_elem:
                    item["content"] = content_elem.get_text().strip()
            
            if selectors.summary:
                summary_elem = soup.select_one(selectors.summary)
                if summary_elem:
                    item["summary"] = summary_elem.get_text().strip()
            
            if selectors.author:
                author_elem = soup.select_one(selectors.author)
                if author_elem:
                    item["author"] = author_elem.get_text().strip()
            
            if selectors.publish_date:
                date_elem = soup.select_one(selectors.publish_date)
                if date_elem:
                    item["published"] = date_elem.get_text().strip()
            
            if selectors.tags:
                tag_elems = soup.select(selectors.tags)
                item["tags"] = [elem.get_text().strip() for elem in tag_elems]
            
            # å¦‚æœæ²¡æœ‰æå–åˆ°æ ‡é¢˜å’Œå†…å®¹ï¼Œè¿”å›None
            if not item.get("title") and not item.get("content"):
                return None
            
            return item
            
        except Exception as e:
            print(f"        å•é¡¹æå–å¤±è´¥: {e}")
            return None
    
    def _extract_item_from_container(self, container, selectors, source: SourceConfig) -> Optional[Dict[str, Any]]:
        """ä»å®¹å™¨ä¸­æå–å†…å®¹é¡¹ç›®"""
        try:
            item = {
                "source": str(source.url),
                "source_name": source.name,
                "type": "webpage",
                "content_type": source.content_type.value
            }
            
            # åœ¨å®¹å™¨å†…æŸ¥æ‰¾å„ä¸ªå­—æ®µ
            if selectors.title:
                title_elem = container.select_one(selectors.title)
                if title_elem:
                    item["title"] = title_elem.get_text().strip()
            
            if selectors.link:
                link_elem = container.select_one(selectors.link)
                if link_elem:
                    href = link_elem.get("href", "")
                    if href:
                        item["url"] = urljoin(str(source.url), href)
            
            if selectors.content:
                content_elem = container.select_one(selectors.content)
                if content_elem:
                    item["content"] = content_elem.get_text().strip()
            
            if selectors.summary:
                summary_elem = container.select_one(selectors.summary)
                if summary_elem:
                    item["summary"] = summary_elem.get_text().strip()
            
            if selectors.author:
                author_elem = container.select_one(selectors.author)
                if author_elem:
                    item["author"] = author_elem.get_text().strip()
            
            if selectors.publish_date:
                date_elem = container.select_one(selectors.publish_date)
                if date_elem:
                    item["published"] = date_elem.get_text().strip()
            
            if selectors.tags:
                tag_elems = container.select(selectors.tags)
                item["tags"] = [elem.get_text().strip() for elem in tag_elems]
            
            # è®¾ç½®é»˜è®¤URL
            if not item.get("url"):
                item["url"] = str(source.url)
            
            # éªŒè¯å¿…è¦å­—æ®µ
            if not item.get("title"):
                return None
            
            return item
            
        except Exception as e:
            print(f"        å®¹å™¨æå–å¤±è´¥: {e}")
            return None
    
    def _calculate_quality_score(self, item: Dict[str, Any]) -> float:
        """è®¡ç®—å†…å®¹è´¨é‡åˆ†æ•°"""
        score = 0.5  # åŸºç¡€åˆ†æ•°
        
        # æ ‡é¢˜è´¨é‡
        title = item.get("title", "")
        if title:
            if len(title) > 10:
                score += 0.1
            if len(title) < 200:  # æ ‡é¢˜ä¸åº”è¯¥å¤ªé•¿
                score += 0.1
        
        # å†…å®¹è´¨é‡
        content = item.get("content", "")
        if content:
            content_length = len(content)
            if content_length > 100:
                score += 0.1
            if content_length > 500:
                score += 0.1
            if content_length < 10000:  # å†…å®¹ä¸åº”è¯¥å¤ªé•¿
                score += 0.1
        
        # å…ƒæ•°æ®å®Œæ•´æ€§
        if item.get("author"):
            score += 0.05
        if item.get("published"):
            score += 0.05
        if item.get("summary"):
            score += 0.1
        if item.get("tags"):
            score += 0.05
        
        return min(1.0, score)
    
    async def _fetch_full_article(self, url: str, source: SourceConfig) -> Optional[str]:
        """è·å–RSSæ¡ç›®çš„å®Œæ•´æ–‡ç« å†…å®¹"""
        try:
            headers = dict(source.auth_config.headers) if source.auth_config.headers else {}
            if not headers.get("User-Agent"):
                headers["User-Agent"] = random.choice(self.user_agents)
            
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=10)) as session:
                async with session.get(url, headers=headers) as response:
                    if response.status == 200:
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        # ä½¿ç”¨ç®€å•çš„å†…å®¹æå–ç­–ç•¥
                        content_selectors = [
                            'article', '.content', '.post-content', 
                            '.entry-content', 'main', '.main'
                        ]
                        
                        for selector in content_selectors:
                            content_elem = soup.select_one(selector)
                            if content_elem:
                                return content_elem.get_text().strip()
                        
                        # å›é€€åˆ°body
                        body = soup.find('body')
                        if body:
                            return body.get_text().strip()[:2000]
        
        except Exception:
            pass
        
        return None