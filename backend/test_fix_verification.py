#!/usr/bin/env python3
"""
éªŒè¯ConfigSectionåºåˆ—åŒ–ä¿®å¤å’Œç°ä»£çˆ¬è™«å·¥å…·çš„æµ‹è¯•è„šæœ¬
"""

import asyncio
import sys
import os
from pathlib import Path

# Add project root to Python path
backend_dir = Path(__file__).parent.absolute()
sys.path.insert(0, str(backend_dir))

from config.settings import get_settings
from services.crawler_service import WebCrawlerService
from services.modern_crawler_service import ModernCrawlerService
from agents.modern_crawler_agent import ModernCrawlerAgent
from agents.base_agent import AgentState
from core.auth_service import MockAuthService


async def test_config_serialization_fix():
    """æµ‹è¯•ConfigSectionåºåˆ—åŒ–ä¿®å¤"""
    print("ğŸ”§ Testing ConfigSection serialization fix...")
    
    try:
        # Load settings
        settings = get_settings()
        
        # Test ConfigSection to_dict conversion
        if hasattr(settings.agents.crawler, 'headers'):
            headers_config = settings.agents.crawler.headers
            print(f"   â€¢ Headers config type: {type(headers_config)}")
            
            # Test safe conversion
            if hasattr(headers_config, 'to_dict'):
                headers_dict = headers_config.to_dict()
                print(f"   âœ… to_dict() method works: {type(headers_dict)}")
            elif hasattr(headers_config, '__dict__'):
                headers_dict = {k: v for k, v in headers_config.__dict__.items() 
                               if not k.startswith('_') and isinstance(v, (str, int, float, bool))}
                print(f"   âœ… __dict__ fallback works: {type(headers_dict)}")
            else:
                print(f"   âš ï¸  Using basic dict conversion")
        else:
            print(f"   â„¹ï¸  No headers config found")
        
        print("   âœ… ConfigSection serialization fix verified")
        return True
        
    except Exception as e:
        print(f"   âŒ ConfigSection test failed: {e}")
        return False


async def test_fixed_crawler_service():
    """æµ‹è¯•ä¿®å¤åçš„çˆ¬è™«æœåŠ¡"""
    print("\nğŸ•·ï¸  Testing fixed crawler service...")
    
    try:
        settings = get_settings()
        auth_service = MockAuthService()
        
        # Test crawler initialization
        async with WebCrawlerService(settings, auth_service) as crawler:
            print("   âœ… Crawler service initialized without serialization errors")
            
            # Test a simple RSS source
            test_source = {
                'id': 'test_rss',
                'url': 'https://ai.googleblog.com/feeds/posts/default',
                'type': 'rss',
                'name': 'Google AI Blog'
            }
            
            # Test source validation (quick check)
            is_valid = await crawler.validate_source(test_source)
            print(f"   â€¢ Source validation: {'âœ… Valid' if is_valid else 'âš ï¸  Invalid'}")
            
            return True
            
    except Exception as e:
        print(f"   âŒ Fixed crawler service test failed: {e}")
        return False


async def test_modern_tools_availability():
    """æµ‹è¯•ç°ä»£å·¥å…·å¯ç”¨æ€§"""
    print("\nğŸ”§ Testing modern tools availability...")
    
    # Test Firecrawl
    try:
        from firecrawl import FirecrawlApp
        firecrawl_available = True
        print("   âœ… Firecrawl: Available")
    except ImportError:
        firecrawl_available = False
        print("   âŒ Firecrawl: Not installed (pip install firecrawl-py)")
    
    # Test Crawl4AI
    try:
        from crawl4ai import AsyncWebCrawler
        crawl4ai_available = True
        print("   âœ… Crawl4AI: Available")
    except ImportError:
        crawl4ai_available = False
        print("   âŒ Crawl4AI: Not installed (pip install crawl4ai)")
    
    # Test Playwright
    try:
        from playwright.async_api import async_playwright
        playwright_available = True
        print("   âœ… Playwright: Available")
    except ImportError:
        playwright_available = False
        print("   âŒ Playwright: Not installed (pip install playwright)")
    
    # Basic tools (always available)
    print("   âœ… aiohttp + BeautifulSoup: Available (fallback)")
    
    total_tools = 4
    available_tools = sum([
        firecrawl_available,
        crawl4ai_available, 
        playwright_available,
        True  # Basic tools always available
    ])
    
    print(f"   ğŸ“Š Tool availability: {available_tools}/{total_tools} ({available_tools/total_tools:.1%})")
    
    return available_tools >= 2  # At least 2 tools should be available


async def test_modern_crawler_agent():
    """æµ‹è¯•ç°ä»£çˆ¬è™«ä»£ç†"""
    print("\nğŸš€ Testing modern crawler agent...")
    
    try:
        # Check for Firecrawl API key
        firecrawl_key = os.getenv('FIRECRAWL_API_KEY')
        if firecrawl_key:
            print("   ğŸ”¥ Firecrawl API key detected")
        else:
            print("   âš ï¸  No Firecrawl API key (using open source tools only)")
        
        # Initialize modern crawler agent
        agent = ModernCrawlerAgent(firecrawl_api_key=firecrawl_key)
        
        # Check tool availability
        availability = agent._check_tool_availability()
        print("   ğŸ”§ Tool availability check:")
        for tool, available in availability.items():
            status = "âœ…" if available else "âŒ"
            print(f"      {status} {tool}")
        
        # Get recommendations for missing tools
        recommendations = agent.get_tool_recommendations()
        if recommendations:
            print("   ğŸ“ Setup recommendations:")
            for tool, advice in recommendations.items():
                print(f"      â€¢ {tool}: {advice[:60]}...")
        
        available_count = sum(availability.values())
        print(f"   ğŸ“Š Available tools: {available_count}/{len(availability)}")
        
        return available_count >= 1  # At least one tool should be available
        
    except Exception as e:
        print(f"   âŒ Modern crawler agent test failed: {e}")
        return False


async def test_quick_crawl():
    """å¿«é€Ÿçˆ¬å–æµ‹è¯• (ä»…æµ‹è¯•1ä¸ªæº)"""
    print("\nâš¡ Quick crawl test (1 source)...")
    
    try:
        settings = get_settings()
        
        # Test with modern crawler service
        async with ModernCrawlerService(settings) as crawler:
            # Override sources for quick test
            crawler.PREMIUM_SOURCES = {
                "rss_feeds": [
                    {
                        "name": "Google AI Blog", 
                        "url": "https://ai.googleblog.com/feeds/posts/default", 
                        "priority": "high"
                    }
                ],
                "websites": []  # Skip websites for quick test
            }
            
            print("   ğŸ•·ï¸  Testing RSS crawl...")
            results = await crawler.crawl_premium_sources()
            
            stats = results.get("stats", {})
            tools_used = results.get("tools_used", [])
            
            print(f"   ğŸ“Š Results:")
            print(f"      â€¢ Total sources: {stats.get('total_sources', 0)}")
            print(f"      â€¢ Successful: {stats.get('successful', 0)}")
            print(f"      â€¢ Articles: {len(results.get('rss_articles', []))}")
            print(f"      â€¢ Tools used: {', '.join(tools_used) if tools_used else 'None'}")
            
            # Show sample article if available
            articles = results.get("rss_articles", [])
            if articles:
                sample = articles[0]
                print(f"   ğŸ“° Sample article: {sample.get('title', 'No title')[:50]}...")
                print(f"      â€¢ Method: {sample.get('extraction_method', 'unknown')}")
                print(f"      â€¢ Quality: {sample.get('quality_score', 0):.2f}")
            
            return stats.get('successful', 0) > 0
            
    except Exception as e:
        print(f"   âŒ Quick crawl test failed: {e}")
        return False


def print_final_report(test_results):
    """æ‰“å°æœ€ç»ˆæµ‹è¯•æŠ¥å‘Š"""
    print("\n" + "=" * 60)
    print("ğŸ“‹ FINAL TEST REPORT")
    print("=" * 60)
    
    passed = sum(test_results.values())
    total = len(test_results)
    
    for test_name, result in test_results.items():
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"   {status} {test_name}")
    
    print(f"\nğŸ“Š Overall Score: {passed}/{total} tests passed ({passed/total:.1%})")
    
    if passed == total:
        print("ğŸ‰ All tests passed! The system is ready for production.")
        print("\nğŸš€ You can now run the enhanced crawler with confidence:")
        print("   â€¢ ConfigSection serialization issues resolved")
        print("   â€¢ Modern tools available and working")
        print("   â€¢ Premium AI sources verified")
    elif passed >= total * 0.7:
        print("âš ï¸  Most tests passed. System is functional with minor issues.")
        print("\nğŸ“ Recommendations:")
        if not test_results.get("ConfigSection Fix", True):
            print("   â€¢ Check configuration file format")
        if not test_results.get("Modern Tools", True):
            print("   â€¢ Install recommended tools: pip install crawl4ai playwright")
        if not test_results.get("Quick Crawl", True):
            print("   â€¢ Check internet connection and source accessibility")
    else:
        print("âŒ Several tests failed. Manual intervention required.")
        print("\nğŸ”§ Troubleshooting needed:")
        for test_name, result in test_results.items():
            if not result:
                print(f"   â€¢ Fix: {test_name}")
    
    print("=" * 60)


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª Enhanced Crawler Fix Verification Test Suite")
    print("Testing ConfigSection fixes and modern tool integration")
    print("=" * 60)
    
    test_results = {}
    
    # Test 1: ConfigSection serialization fix
    test_results["ConfigSection Fix"] = await test_config_serialization_fix()
    
    # Test 2: Fixed crawler service  
    test_results["Fixed Crawler Service"] = await test_fixed_crawler_service()
    
    # Test 3: Modern tools availability
    test_results["Modern Tools"] = await test_modern_tools_availability()
    
    # Test 4: Modern crawler agent
    test_results["Modern Crawler Agent"] = await test_modern_crawler_agent()
    
    # Test 5: Quick crawl test
    test_results["Quick Crawl"] = await test_quick_crawl()
    
    # Print final report
    print_final_report(test_results)
    
    # Return success status
    return sum(test_results.values()) >= len(test_results) * 0.7


if __name__ == "__main__":
    try:
        success = asyncio.run(main())
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\nâ¹ï¸  Test interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\nğŸ’¥ Test suite crashed: {e}")
        sys.exit(1)