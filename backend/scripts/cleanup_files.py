#!/usr/bin/env python3
"""
æ¸…ç†æ— ç”¨çš„æœ¬åœ°æ–‡ä»¶å¤¹è„šæœ¬
æ•°æ®å·²è¿ç§»åˆ° SQLite æ•°æ®åº“ï¼Œå¯ä»¥å®‰å…¨åˆ é™¤æ–‡ä»¶å­˜å‚¨
"""

import sys
import shutil
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
backend_dir = Path(__file__).parent.parent
sys.path.insert(0, str(backend_dir))

def cleanup_storage_files():
    """æ¸…ç†å·²è¿ç§»çš„å­˜å‚¨æ–‡ä»¶"""
    print("ğŸ§¹ å¼€å§‹æ¸…ç†æ— ç”¨çš„æœ¬åœ°æ–‡ä»¶å¤¹...")
    
    data_dir = backend_dir / "data"
    
    # éœ€è¦æ¸…ç†çš„ç›®å½•
    cleanup_dirs = [
        "sources",      # ä¿¡æºé…ç½®ï¼ˆå·²è¿ç§»åˆ°æ•°æ®åº“ï¼‰
        "articles",     # ç”Ÿæˆçš„æ–‡ç« ï¼ˆå·²è¿ç§»åˆ°æ•°æ®åº“ï¼‰ 
        "content",      # å¤„ç†åçš„å†…å®¹ï¼ˆå·²è¿ç§»åˆ°æ•°æ®åº“ï¼‰
        "workflows"     # å·¥ä½œæµçŠ¶æ€ï¼ˆå·²è¿ç§»åˆ°æ•°æ®åº“ï¼‰
    ]
    
    # éœ€è¦ä¿ç•™çš„ç›®å½•
    keep_dirs = [
        "logs"          # æ—¥å¿—æ–‡ä»¶ä¿ç•™
    ]
    
    cleaned_count = 0
    
    for dir_name in cleanup_dirs:
        target_dir = data_dir / dir_name
        if target_dir.exists():
            try:
                # å¤‡ä»½é‡è¦æ–‡ä»¶ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                backup_important_files(target_dir)
                
                # åˆ é™¤ç›®å½•
                shutil.rmtree(target_dir)
                print(f"âœ… å·²æ¸…ç†ç›®å½•: {target_dir}")
                cleaned_count += 1
            except Exception as e:
                print(f"âŒ æ¸…ç†ç›®å½•å¤±è´¥ {target_dir}: {e}")
        else:
            print(f"â­ï¸  ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡: {target_dir}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–æ–‡ä»¶éœ€è¦æ¸…ç†
    if data_dir.exists():
        remaining_files = list(data_dir.glob("*.json"))
        for json_file in remaining_files:
            if should_cleanup_file(json_file):
                try:
                    json_file.unlink()
                    print(f"âœ… å·²æ¸…ç†æ–‡ä»¶: {json_file}")
                    cleaned_count += 1
                except Exception as e:
                    print(f"âŒ æ¸…ç†æ–‡ä»¶å¤±è´¥ {json_file}: {e}")
    
    print(f"âœ… æ¸…ç†å®Œæˆï¼Œå…±æ¸…ç† {cleaned_count} ä¸ªé¡¹ç›®")
    
    # æ˜¾ç¤ºä¿ç•™çš„ç›®å½•
    print("\\nğŸ“ ä¿ç•™çš„ç›®å½•:")
    for dir_name in keep_dirs:
        keep_dir = data_dir / dir_name
        if keep_dir.exists():
            print(f"   {keep_dir}")
    
    # æ˜¾ç¤ºæ•°æ®åº“æ–‡ä»¶
    db_file = data_dir / "app.db"
    if db_file.exists():
        print(f"\\nğŸ’¾ æ•°æ®åº“æ–‡ä»¶: {db_file}")
        print(f"   å¤§å°: {db_file.stat().st_size} å­—èŠ‚")

def backup_important_files(directory: Path):
    """å¤‡ä»½é‡è¦æ–‡ä»¶ï¼ˆå¦‚æœ‰éœ€è¦ï¼‰"""
    # æš‚æ—¶ä¸éœ€è¦å¤‡ä»½ï¼Œå› ä¸ºæ•°æ®å·²ç»è¿ç§»åˆ°æ•°æ®åº“
    pass

def should_cleanup_file(file_path: Path) -> bool:
    """åˆ¤æ–­æ–‡ä»¶æ˜¯å¦åº”è¯¥è¢«æ¸…ç†"""
    filename = file_path.name
    
    # æ¸…ç†æ¨¡å¼åŒ¹é…çš„æ–‡ä»¶
    cleanup_patterns = [
        "crawled_content_*.json",
        "processed_content_*.json", 
        "articles_*.json",
        "workflow_*.json",
        "sources.json"
    ]
    
    for pattern in cleanup_patterns:
        if file_path.match(pattern):
            return True
    
    return False

def main():
    """ä¸»æ¸…ç†æµç¨‹"""
    print("ğŸš€ å¼€å§‹æ¸…ç†è¿ç§»åçš„æ–‡ä»¶...")
    print(f"ğŸ“ åç«¯ç›®å½•: {backend_dir}")
    
    # ç¡®è®¤æ“ä½œ
    print("\\nâš ï¸  è­¦å‘Š: æ­¤æ“ä½œå°†åˆ é™¤å·²è¿ç§»åˆ°æ•°æ®åº“çš„æ–‡ä»¶å­˜å‚¨")
    print("   ç¡®ä¿æ•°æ®åº“è¿ç§»å·²æˆåŠŸå®Œæˆ")
    
    response = input("\\næ˜¯å¦ç»§ç»­æ¸…ç†? (y/N): ").strip().lower()
    if response not in ['y', 'yes']:
        print("âŒ å–æ¶ˆæ¸…ç†æ“ä½œ")
        return
    
    try:
        cleanup_storage_files()
        print("\\nğŸ‰ æ¸…ç†æ“ä½œå®Œæˆ!")
        print("\\nğŸ“Š å½“å‰å­˜å‚¨çŠ¶æ€:")
        print("   âœ… ä¿¡æºé…ç½®: å­˜å‚¨åœ¨ SQLite æ•°æ®åº“")
        print("   âœ… æ–‡ç« æ•°æ®: å­˜å‚¨åœ¨ SQLite æ•°æ®åº“") 
        print("   âœ… å·¥ä½œæµçŠ¶æ€: å­˜å‚¨åœ¨ SQLite æ•°æ®åº“")
        print("   âœ… æ—¥å¿—æ–‡ä»¶: ä¿ç•™åœ¨æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ")
        
    except Exception as e:
        print(f"âŒ æ¸…ç†å¤±è´¥: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()