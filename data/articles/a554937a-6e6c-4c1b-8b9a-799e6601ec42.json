{
  "id": "a554937a-6e6c-4c1b-8b9a-799e6601ec42",
  "title": "On the Impossibility of Separating Intelligence from Judgment: The Computational Intractability of Filtering for AI Alignment",
  "content": "arXiv:2507.07341v1 Announce Type: new \nAbstract: With the increased deployment of large language models (LLMs), one concern is their potential misuse for generating harmful content. Our work studies the alignment challenge, with a focus on filters to prevent the generation of unsafe information. Two natural points of intervention are the filtering of the input prompt before it reaches the model, and filtering the output after generation. Our main results demonstrate computational challenges in filtering both prompts and outputs. First, we show that there exist LLMs for which there are no efficient prompt filters: adversarial prompts that elicit harmful behavior can be easily constructed, which are computationally indistinguishable from benign prompts for any efficient filter. Our second main result identifies a natural setting in which output filtering is computationally intractable. All of our separation results are under cryptographic hardness assumptions. In addition to these core findings, we also formalize and study relaxed mitigation approaches, demonstrating further computational barriers. We conclude that safety cannot be achieved by designing filters external to the LLM internals (architecture and weights); in particular, black-box access to the LLM will not suffice. Based on our technical results, we argue that an aligned AI system's intelligence cannot be separated from its judgment.",
  "url": "https://arxiv.org/abs/2507.07341",
  "source_id": "discovered_-3305257180926427665",
  "category": "artificial_intelligence",
  "author": "Sarah Ball, Greg Gluch, Shafi Goldwasser, Frauke Kreuter, Omer Reingold, Guy N. Rothblum",
  "published_at": null,
  "created_at": "2025-07-11T23:37:36.749816",
  "quality_score": 1.0,
  "metadata": {
    "workflow_id": "6257f28b-72a1-4e50-a95f-fcf8755dcef0",
    "extraction_method": "rss",
    "original_item": {
      "title": "On the Impossibility of Separating Intelligence from Judgment: The Computational Intractability of Filtering for AI Alignment",
      "content": "arXiv:2507.07341v1 Announce Type: new \nAbstract: With the increased deployment of large language models (LLMs), one concern is their potential misuse for generating harmful content. Our work studies the alignment challenge, with a focus on filters to prevent the generation of unsafe information. Two natural points of intervention are the filtering of the input prompt before it reaches the model, and filtering the output after generation. Our main results demonstrate computational challenges in filtering both prompts and outputs. First, we show that there exist LLMs for which there are no efficient prompt filters: adversarial prompts that elicit harmful behavior can be easily constructed, which are computationally indistinguishable from benign prompts for any efficient filter. Our second main result identifies a natural setting in which output filtering is computationally intractable. All of our separation results are under cryptographic hardness assumptions. In addition to these core findings, we also formalize and study relaxed mitigation approaches, demonstrating further computational barriers. We conclude that safety cannot be achieved by designing filters external to the LLM internals (architecture and weights); in particular, black-box access to the LLM will not suffice. Based on our technical results, we argue that an aligned AI system's intelligence cannot be separated from its judgment.",
      "url": "https://arxiv.org/abs/2507.07341",
      "author": "Sarah Ball, Greg Gluch, Shafi Goldwasser, Frauke Kreuter, Omer Reingold, Guy N. Rothblum",
      "published": "Fri, 11 Jul 2025 00:00:00 -0400",
      "category": "\u4eba\u5de5\u667a\u80fd",
      "source_id": "discovered_-3305257180926427665",
      "extraction_method": "rss"
    },
    "topic": "\u4eba\u5de5\u667a\u80fd",
    "topic_relevance": 0.5
  }
}