{
  "id": "47aba7ac-1588-48da-8cb2-6b4b60620286",
  "title": "Linguistic Bias in ChatGPT: Language Models Reinforce Dialect Discrimination",
  "content": "Sample language model responses to different varieties of English and native speaker reactions.\n\nChatGPT does amazingly well at communicating with people in English. But whose English?\nOnly 15% of ChatGPT users are from the US, where Standard American English is the default. But the model is also commonly used in countries and communities where people speak other varieties of English. Over 1 billion people around the world speak varieties such as Indian English, Nigerian English, Irish English, and African-American English.\nSpeakers of these non-\u201cstandard\u201d varieties often face discrimination in the real world. They\u2019ve been told that the way they speak is unprofessional or incorrect, discredited as witnesses, and denied housing\u2013despite extensive research indicating that all language varieties are equally complex and legitimate. Discriminating against the way someone speaks is often a proxy for discriminating against their race, ethnicity, or nationality. What if ChatGPT exacerbates this discrimination?\nTo answer this question, our recent paper examines how ChatGPT\u2019s behavior changes in response to text in different varieties of English. We found that ChatGPT responses exhibit consistent and pervasive biases against non-\u201cstandard\u201d varieties, including increased stereotyping and demeaning content, poorer comprehension, and condescending responses.\n\nOur Study\nWe prompted both GPT-3.5 Turbo and GPT-4 with text in ten varieties of English: two \u201cstandard\u201d varieties, Standard American English (SAE) and Standard British English (SBE); and eight non-\u201cstandard\u201d varieties, African-American, Indian, Irish, Jamaican, Kenyan, Nigerian, Scottish, and Singaporean English. Then, we compared the language model responses to the \u201cstandard\u201d varieties and the non-\u201cstandard\u201d varieties.\nFirst, we wanted to know whether linguistic features of a variety that are present in the prompt would be retained in GPT-3.5 Turbo responses to that prompt. We annotated the prompts and model responses for linguistic features of each variety and whether they used American or British spelling (e.g., \u201ccolour\u201d or \u201cpractise\u201d). This helps us understand when ChatGPT imitates or doesn\u2019t imitate a variety, and what factors might influence the degree of imitation.\nThen, we had native speakers of each of the varieties rate model responses for different qualities, both positive (like warmth, comprehension, and naturalness) and negative (like stereotyping, demeaning content, or condescension). Here, we included the original GPT-3.5 responses, plus responses from GPT-3.5 and GPT-4 where the models were told to imitate the style of the input.\nResults\nWe expected ChatGPT to produce Standard American English by default: the model was developed in the US, and Standard American English is likely the best-represented variety in its training data. We indeed found that model responses retain features of SAE far more than any non-\u201cstandard\u201d dialect (by a margin of over 60%). But surprisingly, the model does imitate other varieties of English, though not consistently. In fact, it imitates varieties with more speakers (such as Nigerian and Indian English) more often than varieties with fewer speakers (such as Jamaican English). That suggests that the training data composition influences responses to non-\u201cstandard\u201d dialects.\nChatGPT also defaults to American conventions in ways that could frustrate non-American users. For example, model responses to inputs with British spelling (the default in most non-US countries) almost universally revert to American spelling. That\u2019s a substantial fraction of ChatGPT\u2019s userbase likely hindered by ChatGPT\u2019s refusal to accommodate local writing conventions.\nModel responses are consistently biased against non-\u201cstandard\u201d varieties. Default GPT-3.5 responses to non-\u201cstandard\u201d varieties consistently exhibit a range of issues: stereotyping (19% worse than for \u201cstandard\u201d varieties), demeaning content (25% worse), lack of comprehension (9% worse), and condescending responses (15% worse).\n\n\n\nNative speaker ratings of model responses. Responses to non-\u201dstandard\u201d varieties (blue) were rated as worse than responses to \u201cstandard\u201d varieties (orange) in terms of stereotyping (19% worse), demeaning content (25% worse), comprehension (9% worse), naturalness (8% worse), and condescension (15% worse).\n\nWhen GPT-3.5 is prompted to imitate the input dialect, the responses exacerbate stereotyping content (9% worse) and lack of comprehension (6% worse). GPT-4 is a newer, more powerful model than GPT-3.5, so we\u2019d hope that it would improve over GPT-3.5. But although GPT-4 responses imitating the input improve on GPT-3.5 in terms of warmth, comprehension, and friendliness, they exacerbate stereotyping (14% worse than GPT-3.5 for minoritized varieties). That suggests that larger, newer models don\u2019t automatically solve dialect discrimination: in fact, they might make it worse.\nImplications\nChatGPT can perpetuate linguistic discrimination toward speakers of non-\u201cstandard\u201d varieties. If these users have trouble getting ChatGPT to understand them, it\u2019s harder for them to use these tools. That can reinforce barriers against speakers of non-\u201cstandard\u201d varieties as AI models become increasingly used in daily life.\nMoreover, stereotyping and demeaning responses perpetuate ideas that speakers of non-\u201cstandard\u201d varieties speak less correctly and are less deserving of respect. As language model usage increases globally, these tools risk reinforcing power dynamics and amplifying inequalities that harm minoritized language communities.\nLearn more here: [ paper ]",
  "url": "http://bair.berkeley.edu/blog/2024/09/20/linguistic-bias/",
  "source_id": "038d31d6-2e17-4a9c-b561-b53734aa9ff8",
  "category": "research",
  "author": "",
  "published_at": null,
  "created_at": "2025-07-01T16:55:18.572973",
  "quality_score": 0.9999999999999999,
  "metadata": {
    "workflow_id": "15333d42-b6a3-4b3f-8c33-91b70037ccf5",
    "extraction_method": "rss",
    "original_item": {
      "title": "Linguistic Bias in ChatGPT: Language Models Reinforce Dialect Discrimination",
      "content": "Sample language model responses to different varieties of English and native speaker reactions.\n\nChatGPT does amazingly well at communicating with people in English. But whose English?\nOnly 15% of ChatGPT users are from the US, where Standard American English is the default. But the model is also commonly used in countries and communities where people speak other varieties of English. Over 1 billion people around the world speak varieties such as Indian English, Nigerian English, Irish English, and African-American English.\nSpeakers of these non-\u201cstandard\u201d varieties often face discrimination in the real world. They\u2019ve been told that the way they speak is unprofessional or incorrect, discredited as witnesses, and denied housing\u2013despite extensive research indicating that all language varieties are equally complex and legitimate. Discriminating against the way someone speaks is often a proxy for discriminating against their race, ethnicity, or nationality. What if ChatGPT exacerbates this discrimination?\nTo answer this question, our recent paper examines how ChatGPT\u2019s behavior changes in response to text in different varieties of English. We found that ChatGPT responses exhibit consistent and pervasive biases against non-\u201cstandard\u201d varieties, including increased stereotyping and demeaning content, poorer comprehension, and condescending responses.\n\nOur Study\nWe prompted both GPT-3.5 Turbo and GPT-4 with text in ten varieties of English: two \u201cstandard\u201d varieties, Standard American English (SAE) and Standard British English (SBE); and eight non-\u201cstandard\u201d varieties, African-American, Indian, Irish, Jamaican, Kenyan, Nigerian, Scottish, and Singaporean English. Then, we compared the language model responses to the \u201cstandard\u201d varieties and the non-\u201cstandard\u201d varieties.\nFirst, we wanted to know whether linguistic features of a variety that are present in the prompt would be retained in GPT-3.5 Turbo responses to that prompt. We annotated the prompts and model responses for linguistic features of each variety and whether they used American or British spelling (e.g., \u201ccolour\u201d or \u201cpractise\u201d). This helps us understand when ChatGPT imitates or doesn\u2019t imitate a variety, and what factors might influence the degree of imitation.\nThen, we had native speakers of each of the varieties rate model responses for different qualities, both positive (like warmth, comprehension, and naturalness) and negative (like stereotyping, demeaning content, or condescension). Here, we included the original GPT-3.5 responses, plus responses from GPT-3.5 and GPT-4 where the models were told to imitate the style of the input.\nResults\nWe expected ChatGPT to produce Standard American English by default: the model was developed in the US, and Standard American English is likely the best-represented variety in its training data. We indeed found that model responses retain features of SAE far more than any non-\u201cstandard\u201d dialect (by a margin of over 60%). But surprisingly, the model does imitate other varieties of English, though not consistently. In fact, it imitates varieties with more speakers (such as Nigerian and Indian English) more often than varieties with fewer speakers (such as Jamaican English). That suggests that the training data composition influences responses to non-\u201cstandard\u201d dialects.\nChatGPT also defaults to American conventions in ways that could frustrate non-American users. For example, model responses to inputs with British spelling (the default in most non-US countries) almost universally revert to American spelling. That\u2019s a substantial fraction of ChatGPT\u2019s userbase likely hindered by ChatGPT\u2019s refusal to accommodate local writing conventions.\nModel responses are consistently biased against non-\u201cstandard\u201d varieties. Default GPT-3.5 responses to non-\u201cstandard\u201d varieties consistently exhibit a range of issues: stereotyping (19% worse than for \u201cstandard\u201d varieties), demeaning content (25% worse), lack of comprehension (9% worse), and condescending responses (15% worse).\n\n\n\nNative speaker ratings of model responses. Responses to non-\u201dstandard\u201d varieties (blue) were rated as worse than responses to \u201cstandard\u201d varieties (orange) in terms of stereotyping (19% worse), demeaning content (25% worse), comprehension (9% worse), naturalness (8% worse), and condescension (15% worse).\n\nWhen GPT-3.5 is prompted to imitate the input dialect, the responses exacerbate stereotyping content (9% worse) and lack of comprehension (6% worse). GPT-4 is a newer, more powerful model than GPT-3.5, so we\u2019d hope that it would improve over GPT-3.5. But although GPT-4 responses imitating the input improve on GPT-3.5 in terms of warmth, comprehension, and friendliness, they exacerbate stereotyping (14% worse than GPT-3.5 for minoritized varieties). That suggests that larger, newer models don\u2019t automatically solve dialect discrimination: in fact, they might make it worse.\nImplications\nChatGPT can perpetuate linguistic discrimination toward speakers of non-\u201cstandard\u201d varieties. If these users have trouble getting ChatGPT to understand them, it\u2019s harder for them to use these tools. That can reinforce barriers against speakers of non-\u201cstandard\u201d varieties as AI models become increasingly used in daily life.\nMoreover, stereotyping and demeaning responses perpetuate ideas that speakers of non-\u201cstandard\u201d varieties speak less correctly and are less deserving of respect. As language model usage increases globally, these tools risk reinforcing power dynamics and amplifying inequalities that harm minoritized language communities.\nLearn more here: [ paper ]",
      "url": "http://bair.berkeley.edu/blog/2024/09/20/linguistic-bias/",
      "author": "",
      "published": "Fri, 20 Sep 2024 02:00:00 -0700",
      "category": "research",
      "source_id": "038d31d6-2e17-4a9c-b561-b53734aa9ff8",
      "extraction_method": "rss"
    }
  }
}