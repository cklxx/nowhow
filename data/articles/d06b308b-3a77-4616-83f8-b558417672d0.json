{
  "id": "d06b308b-3a77-4616-83f8-b558417672d0",
  "title": "Linear Layers and Activation Functions in Transformer Models",
  "content": "This post is divided into three parts; they are: \u2022 Why Linear Layers and Activations are Needed in Transformers \u2022 Typical Design of the Feed-Forward Network \u2022 Variations of the Activation Functions The attention layer is the core function of a transformer model.",
  "url": "https://machinelearningmastery.com/linear-layers-and-activation-functions-in-transformer-models/",
  "source_id": "559b8284-028e-4959-a6f8-831ae277c7aa",
  "category": "ml-tutorials",
  "author": "Adrian Tam",
  "published_at": null,
  "created_at": "2025-07-01T14:50:18.708446",
  "quality_score": 0.9999999999999999,
  "metadata": {
    "workflow_id": "3aa5d9e2-fe9b-4fbe-b06c-e7c5c92ba129",
    "extraction_method": "rss",
    "original_item": {
      "title": "Linear Layers and Activation Functions in Transformer Models",
      "content": "This post is divided into three parts; they are: \u2022 Why Linear Layers and Activations are Needed in Transformers \u2022 Typical Design of the Feed-Forward Network \u2022 Variations of the Activation Functions The attention layer is the core function of a transformer model.",
      "url": "https://machinelearningmastery.com/linear-layers-and-activation-functions-in-transformer-models/",
      "author": "Adrian Tam",
      "published": "Mon, 30 Jun 2025 01:45:34 +0000",
      "category": "ml-tutorials",
      "source_id": "559b8284-028e-4959-a6f8-831ae277c7aa",
      "extraction_method": "rss"
    }
  }
}